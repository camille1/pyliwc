{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API reference\n",
    "\n",
    "> The `LIWC` module provides a Python interface to the *Linguistic Inquiry and Word Count* (LIWC) tool, allowing users to perform text analysis.\n",
    ">> \n",
    "> The module facilitates analysis of various text sources including `CSV` files, folders of text files, `DataFrames`, and individual `strings`. The results can be returned in multiple formats including `DataFrames` and `JSON`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from fastcore.utils import patch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Liwc:\n",
    "    def __init__(self, \n",
    "                 liwc_cli_path: str = 'LIWC-22-cli', # LIWC CLI Path.\n",
    "                 threads: Union[int, None] = None, #Number of threads to use. Defaults to the number of CPU cores minus one.\n",
    "                 verbose: bool = False): # Display printing and progress bar. Defaults to False.\n",
    "        \"\"\"\n",
    "        Initialize the LIWC Class.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.liwc_cli_path = liwc_cli_path\n",
    "        self.threads = str(threads) if threads is not None else str(os.cpu_count() - 1)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def _execute_command(self, cmd_to_execute, verbose: Union[bool, None] = None):\n",
    "        \"\"\"\n",
    "        Execute a command and handle verbose output.\n",
    "\n",
    "        Args:\n",
    "            cmd_to_execute (list): Command to execute as a list of strings.\n",
    "            verbose (bool, optional): Override the instance's verbose setting.\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose is None:\n",
    "            verbose = self.verbose\n",
    "        if self.verbose: # verbose =True\n",
    "            subprocess.call(cmd_to_execute)\n",
    "        else: # verbose False\n",
    "            result = subprocess.run(cmd_to_execute, capture_output=True, text=True)\n",
    "            # if not result.stdout.strip().endswith('temp_out.csv'): # for df do not print temp csv.\n",
    "            #     last_line = result.stdout.strip().split('\\n')[-1]\n",
    "            #     print(last_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIWC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def analyze_csv(self:Liwc, \n",
    "                input_file: str, #Path to the input CSV file.\n",
    "                output_location: str, # Path to save the analysis output.\n",
    "                row_id_indices: str, # Indices of row IDs in the CSV.\n",
    "                column_indices: str, # Indices of text columns in the CSV.\n",
    "                liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze text data from a CSV file using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    input_file = rf'{input_file}'\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", input_file, \n",
    "        \"--row-id-indices\", row_id_indices, \"--column-indices\", column_indices, \n",
    "        \"--output\", output_location, \"--t\", self.threads\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    self._execute_command(cmd_to_execute)\n",
    "\n",
    "@patch\n",
    "def analyze_folder(self:Liwc,\n",
    "                   input_folder: str, # Path to the folder containing text files.\n",
    "                   output_location: str, # Path to save the analysis output.\n",
    "                   liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze all text files in a folder using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    input_folder = rf'{input_folder}'\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", input_folder, \n",
    "        \"--output\", output_location, \"--t\", self.threads\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "\n",
    "    self._execute_command(cmd_to_execute)\n",
    "\n",
    "\n",
    "@patch\n",
    "def analyze_df(self:Liwc, \n",
    "               text: pd.Series, # Pandas Series containing text data.\n",
    "               return_input: bool = False, # Whether to return the input text with the output. Defaults to False.\n",
    "               liwc_dict: str = \"LIWC22\" # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "              ) -> pd.DataFrame: # pd.DataFrame: DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    Analyze text data from a DataFrame using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_csv_file_in = f\"temp_in_{random.randint(0, 999)}.csv\"\n",
    "    temp_csv_file_out = f\"temp_out_{random.randint(0, 999)}.csv\"\n",
    "    text.reset_index().to_csv(temp_csv_file_in, index=False, encoding='utf-8')\n",
    "    self.analyze_csv(temp_csv_file_in, temp_csv_file_out, \"1\", \"2\", liwc_dict=liwc_dict)\n",
    "    result_df = pd.DataFrame()\n",
    "    if return_input:\n",
    "        result_df['text'] = text.values\n",
    "    result_df = pd.concat([result_df, pd.read_csv(temp_csv_file_out, encoding='utf-8')], axis=1).set_index('Row ID')\n",
    "    os.remove(temp_csv_file_in)\n",
    "    os.remove(temp_csv_file_out)\n",
    "    return result_df\n",
    "    \n",
    "def analyze_df2(self:Liwc, \n",
    "               df: pd.DataFrame, # name of the dataframe.\n",
    "               text_col: str, # Name of the column containing text data.\n",
    "               index_col: str, # Index of the df to be returned\n",
    "               return_input: bool = False, # Whether to return the input text with the output. Defaults to False.\n",
    "               liwc_dict: str = \"LIWC22\" # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "              ) -> pd.DataFrame: # pd.DataFrame: DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    Analyze text data from a DataFrame using LIWC.\n",
    "\n",
    "    \"\"\"\n",
    "    temp_csv_file_in = f\"temp_in_{random.randint(0, 999)}.csv\"\n",
    "    temp_csv_file_out = f\"temp_out_{random.randint(0, 999)}.csv\"\n",
    "    df[text_col].reset_index().to_csv(temp_csv_file_in, index=False, encoding='utf-8')\n",
    "    self.analyze_csv(temp_csv_file_in, temp_csv_file_out, \"1\", \"2\", liwc_dict=liwc_dict)\n",
    "    result_df = pd.DataFrame()\n",
    "    if return_input:\n",
    "        result_df[text_col] = df[text_col].values\n",
    "        \n",
    "    if index_col:\n",
    "        result_df[index_col] = df[index_col].values\n",
    "        \n",
    "    result_df = pd.concat([result_df, pd.read_csv(temp_csv_file_out, encoding='utf-8')], axis=1).set_index('Row ID')\n",
    "    os.remove(temp_csv_file_in)\n",
    "    os.remove(temp_csv_file_out)\n",
    "    return result_df\n",
    "    \n",
    "@patch\n",
    "def analyze_string(self:Liwc, \n",
    "                   input_string: str, # The string to analyze.\n",
    "                   output_location: str, # Path to save the analysis output (.csv).\n",
    "                   liwc_dict: str = \"LIWC22\") -> None: # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "    \"\"\"\n",
    "    Analyze a single string using LIWC and save to csv.\n",
    "\n",
    "    \"\"\"\n",
    "    output_location = rf'{output_location}'\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", \"console\", \n",
    "        \"--console-text\", input_string, \"--output\", output_location\n",
    "    ]\n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    subprocess.call(cmd_to_execute)\n",
    "    \n",
    "@patch\n",
    "def analyze_string_to_json(self:Liwc, \n",
    "                           input_string: str, # The string to analyze.\n",
    "                           liwc_dict: str = \"LIWC22\" # Dictionary to use for analysis. Defaults to \"LIWC22\".\n",
    "                          ) -> dict: # Analysis results in JSON format.\n",
    "    \"\"\"\n",
    "    Analyze a single string and return the result as JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: \n",
    "    \"\"\"\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"wc\", \"--input\", \"console\", \n",
    "        \"--console-text\", input_string, \"--output\", \"console\"\n",
    "    ]\n",
    "    \n",
    "    if liwc_dict != \"LIWC22\":\n",
    "        cmd_to_execute.extend([\"--dictionary\", liwc_dict])\n",
    "    \n",
    "    results = subprocess.check_output(cmd_to_execute).strip().splitlines()\n",
    "    return json.loads(results[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liwc = Liwc('LIWC-22-cli.exe', verbose=True)\n",
    "# s = \"As Leclerc entered the Invalides, with his cortege of exaltation in the sun of Africa and the battles of Alsace, enter here, Jean Moulin, with your terrible cortege.\"\n",
    "# r = liwc.analyze_string_to_json(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_keys = ['WC', 'Analytic', 'Clout', 'Authentic', 'Tone']\n",
    "# filtered_dict = {key: r[key] for key in desired_keys if key in r}\n",
    "# print(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# unit tests\n",
    "\n",
    "def test_analyze_csv_creates_output_file(input_file, output_file):\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    # Ensure the output file does not exist before running the function\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    liwc.analyze_csv(input_file=input_file, output_location=output_file, row_id_indices=\"1\", column_indices=\"3\")\n",
    "    \n",
    "    # Use `test` from fastcore.test to check if the output file was created\n",
    "    test_eq(os.path.exists(output_file), True)\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Define the input and output paths\n",
    "input_file = r\"../data/US-president.csv\"\n",
    "output_file = rf\"{random.randint(0, 999)}_output.csv\"\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_csv_creates_output_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_analyze_df_returns_dataframe(input_csv_path):\n",
    "    \"\"\"\n",
    "    Test that the analyze_df function returns a DataFrame with expected properties.\n",
    "    \n",
    "    \"\"\"\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    # Load the test DataFrame\n",
    "    df_test = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    result_df = liwc.analyze_df(df_test)\n",
    "    \n",
    "    # Check that the return value is a DataFrame\n",
    "    test_eq(type(result_df), pd.core.frame.DataFrame)\n",
    "\n",
    "# Define the input CSV path\n",
    "input_csv_path = r\"../data/US-president.csv\"\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_df_returns_dataframe(input_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_analyze_folder_creates_output_file(input_folder, output_location):\n",
    "    \"\"\"\n",
    "    Test that the analyze_folder function creates an output file in the specified output folder.\n",
    "    \"\"\"\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    # Remove all files in the specified directory\n",
    "    [os.remove(os.path.join(output_location, filename)) for filename in os.listdir(output_location) if os.path.isfile(os.path.join(output_location, filename))]\n",
    "\n",
    "    # Call the function to be tested\n",
    "    liwc.analyze_folder(input_folder=input_folder, output_location=output_location)\n",
    "    \n",
    "    # Use `test_eq` from fastcore.test to check if the output file was created\n",
    "    test_eq(os.path.exists(any(filename.endswith('.csv') for filename in os.listdir(output_location))), True)\n",
    "    # Clean up by removing the output file\n",
    "    [os.remove(os.path.join(output_location, filename)) for filename in os.listdir(output_location) if os.path.isfile(os.path.join(output_location, filename))]\n",
    "\n",
    "\n",
    "# Define the input folder and output folder paths\n",
    "input_folder = r\"../data/inaugural-address\"\n",
    "output_location = r\"../data/unittests/\"\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_folder_creates_output_file(input_folder, output_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_analyze_string_creates_output_file(input_string, output_location, liwc_dict='LIWC22'):\n",
    "    \"\"\"\n",
    "    Test that the analyze_string function creates an output file at the specified location.\n",
    "    \n",
    "    \"\"\"\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    output_file = os.path.join(output_location, 'WC-result.csv')\n",
    "    # Ensure the output file does not exist before running the function\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    liwc.analyze_string(input_string=input_string, output_location=output_file, liwc_dict=liwc_dict)\n",
    "    \n",
    "    # Use `test_eq` from fastcore.test to check if the output file was created\n",
    "    test_eq(os.path.exists(output_file), True)\n",
    "    \n",
    "    # Clean up by removing the output file\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Define the input string and output file path\n",
    "input_string = \"This is a test string for LIWC analysis.\"\n",
    "output_location = r\"../data/\"\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_string_creates_output_file(input_string, output_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def test_analyze_string_to_json_returns_dict(input_string, expected_keys, liwc_dict='LIWC22'):\n",
    "    \"\"\"\n",
    "    Test that the analyze_string_to_json function\n",
    "\n",
    "    \"\"\"\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    # Call the function to be tested\n",
    "    result_dict = liwc.analyze_string_to_json(input_string=input_string, liwc_dict=liwc_dict)\n",
    "    \n",
    "    # Check that the return value is a dictionary\n",
    "    test_eq(type(result_dict), dict)\n",
    "    \n",
    "    # Check that the dictionary contains the expected keys\n",
    "    test_eq(set(result_dict.keys()) >= expected_keys, True)\n",
    "\n",
    "    test_eq(len(result_dict), 119)\n",
    "\n",
    "# Define the input string and expected keys in the result dictionary\n",
    "input_string = \"This is a test string for LIWC analysis.\"\n",
    "expected_keys = {'WC', 'Analytic', 'Clout', 'Authentic', 'Tone'}  # Replace with actual expected keys\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_string_to_json_returns_dict(input_string, expected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# local Dictionnaries\n",
    "import collections\n",
    "\n",
    "class _LIWCDictionary:\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes the LIWCDictionary by loading the dictionary file and building the trie.\n",
    "        \n",
    "        :param filepath: Path to the LIWC dictionary file.\n",
    "        \"\"\"\n",
    "        self.categories, self.lexicon = self._load_dict_file(filepath)\n",
    "        self._trie = self._build_char_trie(self.lexicon)\n",
    "\n",
    "    def search(self, word):\n",
    "        \"\"\"\n",
    "        Search a word in the LIWC dictionary.\n",
    "        \n",
    "        :param word: The word to search for.\n",
    "        :return: A list of the LIWC categories the word belongs to, or an empty list if not found.\n",
    "        \"\"\"\n",
    "        return self._search_trie(self._trie, word)\n",
    "\n",
    "    def parse(self, tokens):\n",
    "        \"\"\"\n",
    "        Parses a document and extracts raw counts of words that fall into various LIWC categories.\n",
    "        \n",
    "        :param tokens: A list of tokens from a tokenized document.\n",
    "        :return: A counter with the linguistic categories found in the document and the raw count of words in each category.\n",
    "        \"\"\"\n",
    "        cat_counter = collections.Counter()\n",
    "\n",
    "        for token in tokens:\n",
    "            cats = self.search(token)\n",
    "            for cat in cats:\n",
    "                cat_counter[cat] += 1\n",
    "\n",
    "        return cat_counter\n",
    "\n",
    "    def _load_dict_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Loads the LIWC dictionary file and parses the categories and lexicon.\n",
    "        \n",
    "        :param filepath: Path to the LIWC dictionary file.\n",
    "        :return: A tuple containing the categories dictionary and the lexicon dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as liwc_file:\n",
    "                categories = {}\n",
    "                lexicon = {}\n",
    "                percent_sign_count = 0\n",
    "\n",
    "                for line in liwc_file:\n",
    "                    stp = line.strip()\n",
    "                    if stp:\n",
    "                        parts = stp.split('\\t')\n",
    "                        if parts[0] == '%':\n",
    "                            percent_sign_count += 1\n",
    "                        else:\n",
    "                            if percent_sign_count == 1:\n",
    "                                categories[parts[0]] = parts[1]\n",
    "                            else:\n",
    "                                lexicon[parts[0]] = [categories[cat_id] for cat_id in parts[1:]]\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"The file at {filepath} was not found.\")\n",
    "        except IOError:\n",
    "            raise Exception(f\"An error occurred while reading the file at {filepath}.\")\n",
    "\n",
    "        return categories, lexicon\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_char_trie(lexicon):\n",
    "        \"\"\"\n",
    "        Builds a character trie to handle wildcard ('*') matches.\n",
    "        \n",
    "        :param lexicon: The lexicon dictionary containing words and their categories.\n",
    "        :return: A character trie for the lexicon.\n",
    "        \"\"\"\n",
    "        trie = {}\n",
    "        for pattern, cat_names in lexicon.items():\n",
    "            cursor = trie\n",
    "            for char in pattern:\n",
    "                if char == '*':\n",
    "                    cursor['*'] = cat_names\n",
    "                    break\n",
    "                if char not in cursor:\n",
    "                    cursor[char] = {}\n",
    "                cursor = cursor[char]\n",
    "            cursor['$'] = cat_names\n",
    "        return trie\n",
    "    def get_categories(self):\n",
    "        \"\"\"\n",
    "        Returns the dictionary of categories.\n",
    "    \n",
    "        :return: A dictionary of categories.\n",
    "        \"\"\"\n",
    "        return self.categories\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_lexicon(self):\n",
    "        \"\"\"\n",
    "        Returns the lexicon dictionary.\n",
    "    \n",
    "        :return: A dictionary of words and their associated LIWC categories.\n",
    "        \"\"\"\n",
    "        return self.lexicon\n",
    "\n",
    "\n",
    "    \n",
    "    def get_trie(self):\n",
    "        \"\"\"\n",
    "        Returns the character trie.\n",
    "    \n",
    "        :return: The character trie built from the lexicon.\n",
    "        \"\"\"\n",
    "        return self._trie\n",
    "    \n",
    "    def word_in_categories(self, word, category_list):\n",
    "        \"\"\"\n",
    "        Checks if a word belongs to any of the specified categories.\n",
    "    \n",
    "        :param word: The word to check.\n",
    "        :param category_list: A list of categories to check against.\n",
    "        :return: True if the word belongs to any of the specified categories, False otherwise.\n",
    "        \"\"\"\n",
    "        word_categories = self.search(word)\n",
    "        return any(cat in word_categories for cat in category_list)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _search_trie(trie, token, i=0):\n",
    "        \"\"\"\n",
    "        Searches the given character trie for paths that match the token.\n",
    "        \n",
    "        :param trie: The character trie.\n",
    "        :param token: The token to search for.\n",
    "        :param i: The current index in the token.\n",
    "        :return: A list of categories if the token matches, otherwise an empty list.\n",
    "        \"\"\"\n",
    "        if '*' in trie:\n",
    "            return trie['*']\n",
    "        if '$' in trie and i == len(token):\n",
    "            return trie['$']\n",
    "        if i < len(token):\n",
    "            char = token[i]\n",
    "            if char in trie:\n",
    "                return LIWCDictionary._search_trie(trie[char], token, i + 1)\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "# liwc_dict = LIWCDictionary('path_to_liwc_file.dic')\n",
    "# categories = liwc_dict.search('word')\n",
    "# counts = liwc_dict.parse(['list', 'of', 'tokens'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Style Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#################################################################################\n",
    "########################### LANGUAGE STYLE MATCHING  ############################\n",
    "#################################################################################\n",
    "\n",
    "@patch\n",
    "def analyze_lsm(self:Liwc, \n",
    "                df: pd.DataFrame, \n",
    "                calculate_lsm: str = \"person-and-group\", \n",
    "                group_column: str = 'GroupID', \n",
    "                person_column: str = 'PersonID', \n",
    "                text_column: str = 'Text', \n",
    "                output_type: str = \"pairwise\",\n",
    "                expanded_output: bool = False, \n",
    "                omit_speakers_number_of_turns: int = 0, \n",
    "                omit_speakers_word_count: int = 10, \n",
    "                segmentation: str = \"none\"\n",
    "               ) -> Union[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Analyzes Linguistic Style Matching (LSM) based on the provided DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the text data to be analyzed.\n",
    "    calculate_lsm : str, optional\n",
    "        Sets the type of LSM calculation. Options are:\n",
    "        - \"person\": Calculate only person-level LSM.\n",
    "        - \"group\": Calculate only group-level LSM.\n",
    "        - \"person-and-group\": Calculate both person and group-level LSM.\n",
    "        Default is \"person-and-group\".\n",
    "    group_column : str, optional\n",
    "        The column name in `df` representing the Group ID. Default is 'GroupID'.\n",
    "    person_column : str, optional\n",
    "        The column name in `df` representing the Person ID. Default is 'PersonID'.\n",
    "    text_column : str, optional\n",
    "        The column name in `df` representing the text data. Default is 'Text'.\n",
    "    output_type : str, optional\n",
    "        Sets the type of output. Options are:\n",
    "        - \"one-to-many\": One-to-many comparison.\n",
    "        - \"pairwise\": Pairwise comparison.\n",
    "        Default is \"pairwise\".\n",
    "    expanded_output : bool, optional\n",
    "        Adds an option to get an expanded LSM output. Default is False.\n",
    "    omit_speakers_word_count : int, optional\n",
    "        Omit speakers if the word count is less than this value. Default is 10.\n",
    "    segmentation : str, optional\n",
    "        Segmentation options for splitting the text. Options are:\n",
    "        - \"none\": No segmentation.\n",
    "        - \"not=<number>\": Number of turns per segment.\n",
    "        - \"nofst=<number>\": Number of segments by speaker turn.\n",
    "        - \"nofwc=<number>\": Number of segments by word count.\n",
    "        - \"now=<number>\": Number of words per segment.\n",
    "        - \"boc=<characters>\": Segmentation based on characters.\n",
    "        - \"regexp=<regex>\": Segmentation based on a regular expression.\n",
    "        Default is \"none\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[pd.DataFrame, dict]\n",
    "        The resulting LSM analysis. The output format depends on the specified `output_format`.\n",
    "    \"\"\"\n",
    "    random_number = random.randint(0, 999)\n",
    "    input_path = f'temp_input_{random_number}.csv'\n",
    "    output_dir = 'temp_output_dir'\n",
    "    \n",
    "    # Map keywords to command line parameters\n",
    "    calculate_lsm_map = {\n",
    "        \"person\": 1,\n",
    "        \"group\": 2,\n",
    "        \"person-and-group\": 3\n",
    "    }\n",
    "    output_type_map = {\n",
    "        \"one-to-many\": 1,\n",
    "        \"pairwise\": 2\n",
    "    }\n",
    "    \n",
    "    # Convert keywords to corresponding command line parameters\n",
    "    calculate_lsm_value = calculate_lsm_map.get(calculate_lsm.lower(), 3)\n",
    "    output_type_value = output_type_map.get(output_type.lower(), 1)\n",
    "    \n",
    "    # Map column names to indices\n",
    "    group_column_idx = df.columns.get_loc(group_column) + 1\n",
    "    person_column_idx = df.columns.get_loc(person_column) + 1\n",
    "    text_column_idx = df.columns.get_loc(text_column) + 1\n",
    "    \n",
    "    # Write DataFrame to CSV file\n",
    "    df.to_csv(input_path, index=False)\n",
    "    \n",
    "    # Prepare the command\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"lsm\", \"--input\", input_path, \"--output\", output_dir, \"--output-format\", \"csv\",\n",
    "        \"--calculate-lsm\", str(calculate_lsm_value), \"--group-column\", str(group_column_idx), \"--person-column\", str(person_column_idx),\n",
    "        \"--text-column\", str(text_column_idx), \"--output-type\", str(output_type_value)\n",
    "    ]\n",
    "    \n",
    "    if expanded_output:\n",
    "        cmd_to_execute.append(\"--expanded-output\")\n",
    "    \n",
    "    \n",
    "    if omit_speakers_number_of_turns > 0:\n",
    "        cmd_to_execute.extend([\"--omit-speakers-number-of-turns\", str(omit_speakers_number_of_turns)])\n",
    "    \n",
    "    if omit_speakers_word_count > 0:\n",
    "        cmd_to_execute.extend([\"--omit-speakers-word-count\", str(omit_speakers_word_count)])\n",
    "    \n",
    "    if segmentation != \"none\":\n",
    "        cmd_to_execute.extend([\"--segmentation\", segmentation])\n",
    "    \n",
    "    # Execute the command\n",
    "    self._execute_command(cmd_to_execute)\n",
    "    \n",
    "    # Determine the appropriate output files based on output_type\n",
    "    if output_type_value == 1:\n",
    "        output_files = {\n",
    "            1: \"LSM-Speaker-One-to-many.csv\",\n",
    "            2: \"LSM-Group-One-to-many.csv\"\n",
    "        }\n",
    "    elif output_type_value == 2:\n",
    "        output_files = {\n",
    "            1: \"LSM-Speaker-Pairwise.csv\",\n",
    "            2: \"LSM-Group-Pairwise.csv\"\n",
    "        }\n",
    "    \n",
    "    result = {}\n",
    "    if calculate_lsm_value in [1, 3]:\n",
    "        speaker_file = os.path.join(output_dir, output_files[1])\n",
    "        if os.path.exists(speaker_file):\n",
    "            result['person_level'] = pd.read_csv(speaker_file)\n",
    "    if calculate_lsm_value in [2, 3]:\n",
    "        group_file = os.path.join(output_dir, output_files[2])\n",
    "        if os.path.exists(group_file):\n",
    "            result['group_level'] = pd.read_csv(group_file)\n",
    "    \n",
    "    # Clean up temporary files and directory\n",
    "    os.remove(input_path)\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "    # if calculate_lsm == 3:\n",
    "    #     return result\n",
    "    # elif calculate_lsm == 1:\n",
    "    #     return result.get('person_level')\n",
    "    # elif calculate_lsm == 2:\n",
    "    #     return result.get('group_level')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# unit tests\n",
    "\n",
    "def test_analyze_lsm_returns_dataframe(input_csv_path):\n",
    "    \"\"\"\n",
    "    Test that the analyze_lsm function returns a DataFrame or dictionary with expected properties.\n",
    "    \n",
    "    \"\"\"\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    \n",
    "    # Load the test DataFrame\n",
    "    df_test = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    result = liwc.analyze_lsm(df_test,\n",
    "                              calculate_lsm=\"pairwise\",  \n",
    "                              person_column='President',\n",
    "                              group_column='Party',\n",
    "                              text_column='Text',\n",
    "                              omit_speakers_word_count=1)\n",
    "    \n",
    "    test_eq(isinstance(result, dict), True)\n",
    "\n",
    "    # Check the keys in the result dictionary\n",
    "    if 'person_level' in result:\n",
    "        test_eq(isinstance(result['person_level'], pd.DataFrame), True)\n",
    "    if 'group_level' in result:\n",
    "        test_eq(isinstance(result['group_level'], pd.DataFrame), True)\n",
    "\n",
    "\n",
    "# Define the input CSV path\n",
    "input_csv_path = r\"../data/US-president.csv\"\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "test_analyze_lsm_returns_dataframe(input_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrative arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#################################################################################\n",
    "###########################      Narrative arc       ############################\n",
    "#################################################################################\n",
    "\n",
    "@patch\n",
    "def narrative_arc(self:Liwc, df: pd.DataFrame, column_names: Union[list, None] = None, \n",
    "                  output_individual_data_points: bool = True, scaling_method: str = '0-100', \n",
    "                  segments_number: int = 5, skip_wc: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the narrative arc of text data based on the provided DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the text data to be analyzed.\n",
    "    column_names : Union[list, None], optional\n",
    "        List of column names in `df` that should be processed. If None, all columns are processed. Default is None.\n",
    "    output_individual_data_points : bool, optional\n",
    "        If True, outputs individual data points for each segment. If False, aggregates the data. Default is True.\n",
    "    scaling_method : str, optional\n",
    "        Method for scaling the data. Options are:\n",
    "        - \"0-100\": Scale values between 0 and 100.\n",
    "        - \"Z-score\": Scale values using Z-score normalization.\n",
    "        Default is \"0-100\".\n",
    "    segments_number : int, optional\n",
    "        Number of segments into which the text is divided for analysis. Default is 5.\n",
    "    skip_wc : int, optional\n",
    "        Skip any texts with a word count less than this value. Default is 10.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The resulting DataFrame with the narrative arc analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_number = random.randint(0, 999)\n",
    "    input_path = f'temp_input_{random_number}.csv'\n",
    "    output_path = f'temp_output_{random_number}.csv'\n",
    "    \n",
    "    # Write DataFrame to CSV file\n",
    "    df.to_csv(input_path, index=False)\n",
    "    \n",
    "    # Map column names to indices if column_names is not None\n",
    "    if column_names is not None:\n",
    "        column_indices = [str(df.columns.get_loc(col) + 1) for col in column_names]\n",
    "        column_indices_str = ','.join(column_indices)\n",
    "    else:\n",
    "        column_indices_str = None\n",
    "    \n",
    "    # Map scaling method names to their respective values\n",
    "    scaling_method_map = {\n",
    "        '0-100': 1,\n",
    "        'z-score': 2\n",
    "    }\n",
    "    \n",
    "    if scaling_method.lower() not in scaling_method_map:\n",
    "        raise ValueError(\"Invalid scaling method. Choose '0-100' or 'z-score'.\")\n",
    "    \n",
    "    scaling_method_value = scaling_method_map[scaling_method.lower()]\n",
    "    \n",
    "    # Prepare the command\n",
    "    cmd_to_execute = [\n",
    "        self.liwc_cli_path, \"--mode\", \"arc\", \"--input\", input_path, \"--output\", output_path,\n",
    "        \"--output-format\", \"csv\", \n",
    "        \"--scaling-method\", str(scaling_method_value), \"--segments-number\", str(segments_number), \n",
    "        \"--skip-wc\", str(skip_wc),\n",
    "        \"--output-individual-data-points\", \"yes\" if output_individual_data_points else \"no\"\n",
    "    ]\n",
    "    \n",
    "    if column_indices_str is not None:\n",
    "        cmd_to_execute.extend([\"--column-indices\", column_indices_str])\n",
    "    \n",
    "    # Execute the command\n",
    "    self._execute_command(cmd_to_execute)\n",
    "    \n",
    "    # Read the output into a DataFrame\n",
    "    result_df = pd.read_csv(output_path)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    os.remove(input_path)\n",
    "    os.remove(output_path)\n",
    "    return result_df\n",
    "    \n",
    "@patch\n",
    "def _arc_get_segment_values(self:Liwc, df, prefix):\n",
    "    \"\"\"\n",
    "    Function to extract segment columns and values for each row\n",
    "    \"\"\"\n",
    "    segment_columns = [col for col in df.columns if col.startswith(prefix)]\n",
    "    return df[segment_columns]\n",
    "\n",
    "\n",
    "# Function to plot with an optional legend parameter\n",
    "@patch\n",
    "def plot_narrative_arc(self:Liwc, df: pd.DataFrame, legend_labels: list = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots the narrative arc for the given DataFrame, showing Staging, Plot Progression, and Cognitive Tension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing the narrative arc data.\n",
    "        Note: 'output_individual_data_points=True' in narrative_arc to get all required data to plot the narractive arc.\n",
    "        \n",
    "    legend_labels : list, optional\n",
    "        List of labels for the legend, corresponding to each row in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The resulting plot figure of the narrative arcs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract values for each segment type\n",
    "    staging_values = self._arc_get_segment_values(df, 'Staging_')\n",
    "    plotprog_values = self._arc_get_segment_values(df, 'PlotProg_')\n",
    "    cogtension_values = self._arc_get_segment_values(df, 'CogTension_')\n",
    "\n",
    "    # Determine the number of segments\n",
    "    num_segments = len(staging_values.columns)\n",
    "    segments = range(1, num_segments + 1)\n",
    "\n",
    "    # Check if the number of legend labels matches the number of rows\n",
    "    if legend_labels and len(legend_labels) != len(df):\n",
    "        raise ValueError(\"The number of legend labels must match the number of rows in the DataFrame\")\n",
    "\n",
    "    # Generate a colormap for each subplot\n",
    "    colormaps = [cm.Blues, cm.Greens, cm.Reds]\n",
    "\n",
    "    # Creating the subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plotting Staging\n",
    "    for i, (index, row) in enumerate(staging_values.iterrows()):\n",
    "        color = colormaps[0]((i+0.1)*8 / len(df))\n",
    "        axs[0].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[0].set_title('STAGING')\n",
    "    axs[0].set_xlabel('Segment')\n",
    "    axs[0].set_ylabel('')\n",
    "    axs[0].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[0].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[0].legend()\n",
    "\n",
    "    # Plotting Plot Progression\n",
    "    for i, (index, row) in enumerate(plotprog_values.iterrows()):\n",
    "        color = colormaps[1]((i+0.1)*8 / len(df))\n",
    "        axs[1].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[1].set_title('PLOT PROGRESSION')\n",
    "    axs[1].set_xlabel('Segment')\n",
    "    axs[1].set_ylabel('')\n",
    "    axs[1].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[1].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[1].legend()\n",
    "\n",
    "    # Plotting Cognitive Tension\n",
    "    for i, (index, row) in enumerate(cogtension_values.iterrows()):\n",
    "        color = colormaps[2]((i+0.1)*8 / len(df))\n",
    "        axs[2].plot(segments, row, marker='o', color=color, linewidth=2, \n",
    "                    markeredgewidth=2, markerfacecolor='white', markeredgecolor=color, \n",
    "                    label=legend_labels[i] if legend_labels else None)\n",
    "    axs[2].set_title('COGNITIVE TENSION')\n",
    "    axs[2].set_xlabel('Segment')\n",
    "    axs[2].set_ylabel('')\n",
    "    axs[2].set_ylim(-5, 110)  # Adding some padding to the bottom\n",
    "    axs[2].set_xticks(segments)  # Ensuring x-ticks are whole numbers\n",
    "    if legend_labels:\n",
    "        axs[2].legend()\n",
    "\n",
    "    # Adjusting layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# unit test\n",
    "\n",
    "def test_narrative_arc_returns_dataframe(input_csv_path):\n",
    "    \"\"\"\n",
    "    Test that the narrative_arc function returns a DataFrame or dictionary with expected properties.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    \n",
    "    # Load the test DataFrame\n",
    "    df_test = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    result = liwc.narrative_arc(\n",
    "        df=df_test, \n",
    "        column_names=['Text'], \n",
    "        output_individual_data_points=True, \n",
    "        scaling_method='0-100', \n",
    "        segments_number=5, \n",
    "        skip_wc=10\n",
    "    )\n",
    "    \n",
    "    test_eq(isinstance(result, pd.DataFrame), True)\n",
    "\n",
    "# Define the input CSV path\n",
    "input_csv_path = r\"../data/US-president.csv\"\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "test_narrative_arc_returns_dataframe(input_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# unit tests for figure\n",
    "\n",
    "def test_plot_narrative_arc_returns_figure(input_csv_path):\n",
    "    \"\"\"\n",
    "    Test that the plot_narrative_arc function returns a figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    liwc = Liwc('LIWC-22-cli.exe')\n",
    "    \n",
    "    # Load the test DataFrame\n",
    "    df_test = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Perform the narrative arc analysis\n",
    "    narrative_results = liwc.narrative_arc(\n",
    "        df=df_test, \n",
    "        column_names=['Text'], \n",
    "        output_individual_data_points=True, \n",
    "        scaling_method='0-100', \n",
    "        segments_number=5, \n",
    "        skip_wc=10\n",
    "    )\n",
    "    \n",
    "    # Call the function to be tested\n",
    "    fig = liwc.plot_narrative_arc(narrative_results)\n",
    "    \n",
    "    # Check if the returned object is a matplotlib figure\n",
    "    test_eq(isinstance(fig, plt.Figure), True)\n",
    "\n",
    "# Define the input CSV path\n",
    "input_csv_path = r\"../data/US-president.csv\"\n",
    "\n",
    "# Run the test function\n",
    "test_plot_narrative_arc_returns_figure(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
